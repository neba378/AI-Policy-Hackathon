# ğŸš€ Quick Start Guide - AI Documentation Scraper

## âš¡ 5-Minute Setup

### Step 1: Install Dependencies
```bash
npm install
```

### Step 2: Configure MongoDB
Create `.env` file in project root:
```env
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/
NODE_ENV=production
```

### Step 3: View Available Sources
```bash
npm run sources
```

### Step 4: Test with Single Source
```bash
npm run start:test
```

### Step 5: Run Production Scraper
```bash
npm start
```

---

## ğŸ“‹ Command Reference

| Command | Description | Use Case |
|---------|-------------|----------|
| `npm run sources` | List all 37 configured sources | View what's available |
| `npm run start:test` | Process 1 source (~30 sec) | Quick test/validation |
| `npm start` | Process high-priority sources (15) | **Recommended** for production |
| `npm run start:all` | Process ALL 37 sources (~2 hrs) | Complete documentation index |
| `npm run start:openai` | Process OpenAI sources only | Company-specific scraping |
| `npm run start:anthropic` | Process Anthropic sources only | Company-specific scraping |
| `npm run start:google` | Process Google sources only | Company-specific scraping |
| `npm run start:meta` | Process Meta sources only | Company-specific scraping |
| `npm run query` | Query stored data in MongoDB | View scraping results |

---

## ğŸ—ºï¸ System Navigation

### Main Entry Points

```
start-ai-scraper-direct.js     â† START HERE: Main scraper
â”‚
â”œâ”€â†’ src/core/processor.js      â† Orchestrates entire pipeline
â”‚   â”œâ”€â†’ src/scrapers/aiDocScraper.js   â† Document scraping logic
â”‚   â”‚   â”œâ”€â†’ src/parsers/pdfParser.js   â† PDF extraction
â”‚   â”‚   â”œâ”€â†’ src/parsers/webParser.js   â† Web scraping
â”‚   â”‚   â””â”€â†’ src/parsers/githubParser.js â† GitHub repos
â”‚   â”‚
â”‚   â”œâ”€â†’ src/processors/textChunker.js  â† Text chunking
â”‚   â”œâ”€â†’ src/core/embeddingService.js   â† Generate embeddings
â”‚   â””â”€â†’ src/core/database.js           â† MongoDB storage
â”‚
â”œâ”€â†’ show-sources.js            â† View configurations
â””â”€â†’ query-database.js          â† Query results
```

### Configuration Files

```
src/config/aiDocsConfig.js     â† 37 AI model sources
.env                            â† MongoDB connection
package.json                    â† Scripts & dependencies
```

### Key Directories

```
src/
â”œâ”€â”€ core/          â† Core processing logic
â”œâ”€â”€ scrapers/      â† Scraping engines
â”œâ”€â”€ parsers/       â† Document parsers (PDF, web, GitHub)
â”œâ”€â”€ processors/    â† Text processing (chunking)
â”œâ”€â”€ config/        â† Source configurations
â””â”€â”€ utils/         â† Logging, retry logic
```

---

## ğŸ”„ Processing Flow

```
1. Load Config
   â”œâ”€â†’ aiDocsConfig.js (37 sources)
   â””â”€â†’ Select sources (--test, --all, --openai, etc.)

2. Connect MongoDB
   â”œâ”€â†’ .env MONGODB_URI
   â””â”€â†’ Separate DB per company (openai, anthropic, google...)

3. Initialize Embeddings
   â”œâ”€â†’ Download model (~90MB, first run only)
   â””â”€â†’ Load paraphrase-multilingual-MiniLM-L12-v2

4. Process Each Source
   â”œâ”€â†’ Parse document (PDF/Web/GitHub)
   â”œâ”€â†’ Chunk text (500-1000 tokens/chunk)
   â”œâ”€â†’ Generate embeddings (384-dim vectors)
   â””â”€â†’ Store in MongoDB

5. Results
   â”œâ”€â†’ Separate collection per model
   â”œâ”€â†’ Full metadata preserved
   â””â”€â†’ Query with npm run query
```

---

## ğŸ¯ Common Workflows

### Test the System
```bash
# 1. Check what's configured
npm run sources

# 2. Test with 1 source
npm run start:test

# 3. Query results
npm run query
```

### Production Scraping
```bash
# Scrape high-priority sources (15 sources, ~30-45 min)
npm start

# Monitor progress in logs/
tail -f logs/combined.log
```

### Company-Specific Scraping
```bash
# Scrape all OpenAI docs (16 sources)
npm run start:openai

# Scrape all Anthropic docs (5 sources)
npm run start:anthropic

# Scrape specific company then query
npm run start:google && npm run query
```

### Complete Index
```bash
# Scrape everything (37 sources, ~1-2 hours)
npm run start:all

# Query all data
npm run query
```

---

## ğŸ“Š Understanding Output

### During Scraping
```bash
[INFO] Processing 1/15: OpenAI/GPT-4 - https://cdn.openai.com/papers/...
[INFO] âœ… Success: 256 chunks processed for OpenAI/GPT-4
[INFO] âœ… Stored 256 chunks to database for OpenAI/GPT-4
```

### After Completion
```bash
[INFO] âœ… Processing Complete!
[INFO] â±ï¸  Duration: 1234s
[INFO] ğŸ“Š Total: 15
[INFO] âœ… Successful: 15
[INFO] âŒ Failed: 0
[INFO] ğŸ“Š Database Statistics:
[INFO]    Total chunks: 3842
[INFO]    Databases: 5
[INFO]    Collections: 15
```

---

## ğŸ—„ï¸ Database Structure

### After Running OpenAI Scraper
```
MongoDB Atlas
â””â”€â”€ openai/                     # Database for OpenAI
    â”œâ”€â”€ gpt_4_chunks           # 256 chunks
    â”œâ”€â”€ gpt_4o_chunks          # 106 chunks
    â”œâ”€â”€ o1_chunks              # 198 chunks
    â””â”€â”€ ...
```

### Query Results
```bash
npm run query

# Example output:
Company: openai
Model: gpt-4o
Total Chunks: 106
Policy Categories: safety, capabilities, performance
Document Type: System Card
Source: https://cdn.openai.com/gpt-4o-system-card.pdf
```

---

## ğŸ› ï¸ Troubleshooting

### MongoDB Connection Failed
```bash
# Check .env file exists and has MONGODB_URI
cat .env

# Test connection
node -e "require('dotenv').config(); console.log(process.env.MONGODB_URI)"
```

### No Data After Scraping
```bash
# Check logs
cat logs/combined.log | grep "ERROR"

# Verify MongoDB connection
npm run query
```

### Embedding Model Download Slow
```bash
# First run downloads ~90MB model
# Progress shown:
# ğŸ”„ Loading embedding model (multilingual)...
# âœ… Embedding model loaded successfully

# Subsequent runs use cached model (fast)
```

---

## ğŸ“ File Locations

| File/Folder | Purpose | When to Check |
|-------------|---------|---------------|
| `logs/combined.log` | All logs | Debugging, monitoring |
| `logs/error.log` | Error logs only | When something fails |
| `.env` | MongoDB credentials | Connection issues |
| `src/config/aiDocsConfig.js` | Source configs | Add new sources |
| `node_modules/` | Downloaded model cache | After first run |

---

## ğŸ“ Learning Path

1. **Beginner**: `npm run start:test` â†’ Understand basic flow
2. **Intermediate**: `npm run start:openai` â†’ Company-specific scraping
3. **Advanced**: `npm run start:all` â†’ Full production index
4. **Expert**: Modify `src/config/aiDocsConfig.js` â†’ Add custom sources

---

## ğŸ’¡ Pro Tips

- **Start small**: Always test with `npm run start:test` first
- **Monitor logs**: Keep `logs/combined.log` open during scraping
- **Check results**: Run `npm run query` after each scrape
- **Incremental**: Scrape by company instead of all at once
- **Backup**: Database auto-creates collections, no manual setup needed

---

## ğŸš¨ Emergency Commands

```bash
# Stop scraping
Ctrl+C

# Clear logs
rm -rf logs/*.log

# Reinstall dependencies
rm -rf node_modules && npm install

# Reset everything except .env
git clean -fdx -e .env
npm install
```

---

## ğŸ“ Need Help?

- Check logs: `logs/combined.log`
- View README: `README.md`
- Check sources: `npm run sources`
- Test connection: `npm run start:test`

---

**Ready to start?** Run:
```bash
npm run start:test
```

Then check results:
```bash
npm run query
```

ğŸ‰ **You're all set!**
